# -*- coding: utf-8 -*-
"""ML2025_HW9_Model_Merging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17CMSi43f2AS8Hc2Dr19hQlipMvFJu8xq

# **HW9 Model Merging**
This is the Colab Notebook of ML2025 HW9.

# Load Dataset from huggingface
Save GSM8K.json and ARC.json on Colab for inference.
"""

# Load Dataset from huggingface
from datasets import load_dataset
import json

# This loads the default config with GSM8K and ARC as splits
dataset = load_dataset("MonicaHuang/ML2025_HW9")

# Access the splits
gsm8k = dataset["GSM8K"]
arc = dataset["ARC"]
# gsm8k_path = "/content/GSM8K.json"
# arc_path = "/content/ARC.json"
gsm8k_path = "GSM8K.json"
arc_path = "ARC.json"
gsm8k_list = gsm8k.to_list()
arc_list = arc.to_list()

# Save datasets locally to Colab files
with open(gsm8k_path, "w") as f:
    json.dump(gsm8k_list, f, indent=2)

with open(arc_path, "w") as f:
    json.dump(arc_list, f, indent=2)

"""# Update peft package and install on Colab
- There are two choices for modifying peft package. You can either:
  1. Generate a private peft github repo from TA-version peft repo (https://github.com/chenjoachim/peft-ml2025-hw9.git `Use this template`), clone and modify certain .py modules in peft to include your own merging methods in the package. Push your local modification to remote (private repo).
    - Remember to generate your private repo access token and `pip install git+https://{username}:{token}@github.com/{username}/{private_peft_repo_name}.git`
    - How to build a private customized peft package for accessing modified peft package on Colab/Kaggle?  
    Tutorial Slide: https://docs.google.com/presentation/d/1tScnnXok48IBxnvQziysbv_johUdvdujsEQZrlk6xTU/edit?usp=sharing
  2. Clone TA-version peft package, modify on Colab and install in editable mode (not recommend).
"""


"""# Import Packages"""

# Commented out IPython magic to ensure Python compatibility.
# Set deterministic cuBLAS behavior before importing torch
# %env CUBLAS_WORKSPACE_CONFIG=:4096:8
import os
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

# Import Packages
import json
import itertools
import torch
import re
import numpy as np
import wandb
import argparse
import string

from itertools import zip_longest
from tqdm import tqdm
from peft import PeftModel
from transformers import (
    GenerationConfig,
    AutoModelForCausalLM,
    AutoTokenizer
)
import torch
from peft import LoraConfig
from transformers import BitsAndBytesConfig

"""# Prompt Generation


> You're not allowed to modify instructions, questions and options in HW9!


"""

# Prompt Generation, Data Preprocessor

instruction_dict = {
    "GSM8K": "You are given a math question and four answer options (associated with \"A\", \"B\", \"C\", \"D\"). Your task is to carefully analyze the problem, apply logical reasoning, and select the correct answer. There is only one correct answer for each question.",
    "ARC": "You are given a science question and four answer options (associated with \"A\", \"B\", \"C\", \"D\"). Your task is to find the correct answer based on scientific facts, knowledge, and reasoning. There is only one correct answer for each question.",
}

def mcqa_prompt(task_name, instruction, question, options):
    """
    Constructs a multiple-choice question answering (MCQA) prompt in instruction-tuning format.

    Args:
        task_name (str): The name of the task (e.g., "GSM8K", "ARC").
        instruction (str): A task-specific instruction (not used directly in this function).
        question (str): The question to be answered.
        options (dict): A dictionary of answer options where keys are option IDs (e.g., "A", "B") and values are option texts.

    Returns:
        str: A formatted instruction-tuning prompt with system message, instruction, and user input.
    """

    sys_msg = instruction_dict[task_name]
    options_dict = options
    IDs = ['A', 'B', 'C', 'D', 'E', 'F']
    if "D" in options_dict:
        op_ids = [IDs[i] for i in range(4)]
    else:
        op_ids = [IDs[i] for i in range(3)]
    options = [options_dict[op_id] for op_id in op_ids]

    user_prompt = f"Question: {question}; Options: " + \
        " ".join([f"({op_id}) {option}" for op_id, option in zip(op_ids, options)])

    return f"""[INST] <<SYS>>You are a helpful assistant and good at solving tasks based on task instructions and inputs.<</SYS>> Instruction: {sys_msg} Input: {user_prompt}[/INST]"""

def generate_prompt(task_name, tokenizer, data_point):
    """
    Generates a final prompt for the model by wrapping task-specific question and options.

    Args:
        task_name (str): The name of the dataset/task (e.g., "GSM8K", "ARC").
        tokenizer: A tokenizer object (not used in this function but included for compatibility).
        data_point (dict): A single example containing keys "instruction", "question", and "options".

    Returns:
        str: A fully constructed prompt ready to be tokenized and passed to the model.
    """

    return mcqa_prompt(
      task_name = task_name,
      instruction = data_point["instruction"],
      question = data_point["question"],
      options = data_point["options"],
    )

"""# Seed Settings"""

# Sets the CUDA device to use
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
device = torch.device('cuda:0')

# Seed Settings
torch.manual_seed(42)
torch.use_deterministic_algorithms(True)
torch.backends.cudnn.benchmark = False
np.random.seed(42)

"""# Merging Settings

"""

# Merge Setting Constants and Paths
# Current Merging Settings, including merging algorithm, density, or weights

#### TODO ####
# MERGE_TYPE = "sce" # "ties", "linear", "magnitude_prune", "dare_ties", "dare_linear", "sce"
# density = 0.4
# weight_math = 1.0
# weight_science = 0.2

MERGE_TYPE = "magnitude_prune" # "ties", "linear", "magnitude_prune", "dare_ties", "dare_linear", "sce"
density = 0.4
weight_math = 1.0
weight_science = 0.26

# MERGE_TYPE = "dare_linear" # "ties", "linear", "magnitude_prune", "dare_ties", "dare_linear", "sce"
# density = 0.4
# weight_math = 1.0
# weight_science = 0.1

if weight_math and weight_science:
  weights = [weight_math, weight_science]
else:
  weights = None

OUTPUT_DIR = "outputs"

# Modification is not recommended, as it is related to merging model weights.
MERGE_TASK_NAMES = ["GSM8K", "ARC"]

"""# Model Configs"""

# Model Configs
# Do not modify!
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)
lora_config = LoraConfig(
    r=8,
    target_modules=["q_proj", 'k_proj', "v_proj"],
    task_type="CAUSAL_LM",
    lora_alpha=16,
    lora_dropout=0.05
)
RANDOM_SEED = 42
CUTOFF_LEN = 700

"""# Load Models and Tokenizers
LoRA Weights of two tasks are loaded from huggingface.


"""

# Load Models, Adapters and Tokenizers
print("Load Model and Tokenizer ...")

base_model_name = "unsloth/llama-2-7b-chat-bnb-4bit"
tokenizer = AutoTokenizer.from_pretrained(base_model_name,
        quantization_config=quant_config, use_fast=True)

tokenizer.pad_token_id = 1
tokenizer.eos_token_id = 2

# Load base model
model = AutoModelForCausalLM.from_pretrained(base_model_name,
        quantization_config=quant_config, low_cpu_mem_usage=True, torch_dtype=torch.float16).to(device)
model.resize_token_embeddings(len(tokenizer))

adapters = MERGE_TASK_NAMES
print(f"Adapters: {adapters}")

hf_model_dict = {
    "GSM8K": "MonicaHuang/llama-2-7b-chat-GSM8K-MCQA",
    "ARC": "chenjoachim/llama-2-7b-chat-ARC-MCQA",
}

# Load LoRA adapters, i.e. task vectors of two tasks
model = PeftModel.from_pretrained(model, hf_model_dict[adapters[0]], adapter_name=adapters[0]).to(device)
for adapter in adapters[1:]:
    _ = model.load_adapter(hf_model_dict[adapter], adapter_name=adapter, device=device)

"""---


# Reminders
> Make sure to inference questions of two tasks under the same merging setting and Generation Config. Don't change settings or configs during the whole inference process.

# Merging with a specific algorithm in peft package
"""

# Switch between Merging Algorithms
weights = weights
adapter_name ="merge"
density = density

########
#### TODO: append newly integrated methods here (to access peft package) ####
########

if MERGE_TYPE == "ties":
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="ties", density=density)
    model.set_adapter("merge")
elif MERGE_TYPE == "linear": # task_arithmetic
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="linear")
    model.set_adapter("merge")
elif MERGE_TYPE == "magnitude_prune":
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="magnitude_prune", density=density)
    model.set_adapter("merge")
elif MERGE_TYPE == "dare_ties":
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="dare_ties", density=density)
    model.set_adapter("merge")
elif MERGE_TYPE == "dare_linear":
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="dare_linear", density=density)
    model.set_adapter("merge")
elif MERGE_TYPE == "sce":
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="sce", density=density)
    model.set_adapter("merge")



"""# Generation Configs"""

#######
#### TODO: GenerationConfig - Hyperparameter Tuing ####
#######
# Decoding Strategy and Max New Length of Token Generation
hyperparameters = {
    "do_sample": False,
    "temperature": None,
    "num_beams": 3,
    "top_p": None,
    "no_repeat_ngram_size": 3,
    "max_new_len": 400
}
# hyperparameters = {
#     "do_sample": True,
#     "temperature": 0.3,
#     "num_beams": 3,
#     "top_p": 0.9,
#     "no_repeat_ngram_size": 3,
#     "max_new_len": 400
# }
generation_config = GenerationConfig(
    do_sample=hyperparameters["do_sample"],
    temperature=hyperparameters["temperature"],
    top_p=hyperparameters["top_p"],
    num_beams=hyperparameters["num_beams"],
    pad_token_id=1,
    max_new_tokens=hyperparameters["max_new_len"]
)

print("Finish setting Generation Config ...")

"""---

# Math - GSM8K Inference Pipeline
"""

# GSM8K Inference Pipeline
TEST_TASK = "GSM8K"
TEST_SET_PATH = f"{TEST_TASK}.json"

print(f"Load Test Dataset {TEST_TASK} ...")
with open(TEST_SET_PATH, "r", encoding = "utf-8") as f:
    test_datas = json.load(f)

print(f"Inference ... {TEST_TASK}")

results = []
max_iteration = len(test_datas)
for (i, test_data) in tqdm(enumerate(test_datas[:max_iteration]), total = max_iteration):

    """ ====== Inference ======= """
    prompt = generate_prompt(TEST_TASK, tokenizer, test_data)

    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=True)
    input_ids = inputs["input_ids"].cuda()

    model.eval()
    with torch.no_grad():
      generation_output = model.generate(
          input_ids=input_ids,
          generation_config=generation_config,
          return_dict_in_generate=True,
          output_scores=True,
          max_new_tokens=hyperparameters["max_new_len"],
      )

    for s in generation_output.sequences:
        predict = tokenizer.decode(s)

    response = predict.split("[/INST]")[-1].split('</s>')[0].strip()
    #### Hints: output some generated responses ####
    # print(f"response: {response}")
    results.append({
        "id": test_data["id"],
        "response": response,
    })
    """ ====== Inference End ====== """

# Save Response Records
"""====== Records ======"""
if weights:
  wnames = '_'.join([str(float(w)) for w in weights])
else:
  wnames = None

result_dir = f"{OUTPUT_DIR}/{MERGE_TYPE}/{TEST_TASK}" # e.g. outputs/magnitude_prune/GSM8K

if not os.path.exists(result_dir):
    os.makedirs(result_dir, exist_ok=True)

# e.g. outputs/magnitude_prune/GSM8K/w_1.0_0.4_d_0.2_result.json
with open(f"{result_dir}/w_{wnames}_d_{density}_result.json","w", encoding = "utf-8") as f:
    json.dump(results, f, indent = 2, ensure_ascii = False)

"""# Science - ARC Inference Pipeline"""

# ARC Inference Pipeline
TEST_TASK = "ARC"
TEST_SET_PATH = f"{TEST_TASK}.json"

""" ====== Load Test Dataset ====== """
print(f"Load Test Dataset {TEST_TASK} ...")
with open(TEST_SET_PATH, "r", encoding = "utf-8") as f:
    test_datas = json.load(f)

print(f"Inference ...{TEST_TASK}")

results = []
max_iteration = len(test_datas)
for (i, test_data) in tqdm(enumerate(test_datas[:max_iteration]), total = max_iteration):

    """ ====== Inference ======= """
    prompt = generate_prompt(TEST_TASK, tokenizer, test_data)

    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=True)
    input_ids = inputs["input_ids"].cuda()

    model.eval()
    with torch.no_grad():
      generation_output = model.generate(
          input_ids=input_ids,
          generation_config=generation_config,
          return_dict_in_generate=True,
          output_scores=True,
          max_new_tokens=hyperparameters["max_new_len"],
      )

    for s in generation_output.sequences:
        predict = tokenizer.decode(s)

    response = predict.split("[/INST]")[-1].split('</s>')[0].strip()
    #### Hints: output some generated responses ####
    # print(f"response: {response}")
    results.append({
        "id": test_data["id"],
        "response": response
    })
    """ ====== Inference End ====== """


"""====== Records ======"""
if weights:
  wnames = '_'.join([str(float(w)) for w in weights])
else:
  wnames = None
result_dir = f"{OUTPUT_DIR}/{MERGE_TYPE}/{TEST_TASK}"

if not os.path.exists(result_dir):
    os.makedirs(result_dir, exist_ok=True)

with open(f"{result_dir}/w_{wnames}_d_{density}_result.json","w", encoding = "utf-8") as f:
    json.dump(results, f, indent = 2, ensure_ascii = False)

"""# Generate Judgeboi Submission File

The prediction file (pred.json) must follow this structure:
- Root must be a dictionary ({}).
- Each key must be a string representing an ID (e.g., "arc_1", "gsm8k_32").
- Each value must be a string containing the model-generated response without input prompt.

"""

# Generate Judgeboi Submission files
# 400 responses in pred.json
import json
#### TODO: The response generation of two tasks must be under same merging setting!!!!! ####
arc_path = f"outputs/{MERGE_TYPE}/ARC/w_{weight_math}_{weight_science}_d_{density}_result.json" # Update the path with your arc file path(absolute)
gsm8k_path = f"outputs/{MERGE_TYPE}/GSM8K/w_{weight_math}_{weight_science}_d_{density}_result.json" # Update the path with your gsm8k file path(absolute)
output_path = "pred.json" # name of output file is fixed

def load_list_as_dict(filepath):
    """
    Loads a list of {"id": ..., "response": ...} and converts it to {id: response}
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    output = {}
    for item in data:
        if 'id' in item and 'response' in item:
            output[item['id']] = item['response']
        else:
            print(f"[Error] Skipping item without valid 'id' and 'response': {item}")
    return output

gsm8k_dict = load_list_as_dict(gsm8k_path)
arc_dict = load_list_as_dict(arc_path)

combined = {**gsm8k_dict, **arc_dict}

with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(combined, f, indent=2, ensure_ascii=False)

"""## End of HW9 Notebook
Thank you for reading!
"""

# Appendix: Batch Decoding/Generation
'''
def evaluate_batch(task_name, model, tokenizer, data_points, generation_config, max_len, verbose=True):

    prompts = [generate_prompt(task_name, tokenizer, dp) for dp in data_points]

    # Batch tokenization
    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=CUTOFF_LEN)
    input_ids = inputs["input_ids"].cuda()

    model.eval()
    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_len,
        )

    outputs = tokenizer.batch_decode(generation_output.sequences, skip_special_tokens=True)

    if verbose:
        for idx, output in enumerate(outputs):
            print("================= Response of Model ==================")
            print({
                "input_prompt": prompts[idx],
                "output_id": generation_output.sequences[idx],
                "output": output,
            })

    return outputs

tokenizer.padding_side = "left"
config = {
    "batch_size": 4
}
results = []
max_iteration = len(test_datas)
BATCH_SIZE = config["batch_size"]

for batch_start in tqdm(range(0, max_iteration, BATCH_SIZE), total=(max_iteration // BATCH_SIZE)+1):
    batch_data = test_datas[batch_start:batch_start+BATCH_SIZE]

    predicts = evaluate_batch(
        TEST_TASK,
        model,
        tokenizer,
        batch_data,
        generation_config,
        hyperparameters["max_new_len"],
        verbose = False
    )

    for predict, test_data in zip(predicts, batch_data):
        response = predict.split("[/INST]")[-1].split('</s>')[0].strip()


        results.append({
            "id": test_data["id"],
            "response": response
        })
        """ ====== Inference End ====== """
'''
# -*- coding: utf-8 -*-
"""ML2025-HW10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V-gvtTbuqWCvIv7Ox4NE9gb_HiMEplVz

# ML2025 Homework 10 - Diffusion

This notebook is for ML2025 Homework 10, focusing on the customization of diffusion model. The goal is to use BLIP Diffusion and Custom Diffusion to generate images with custom objects.

Codes in this notebook is modified from [ref1](https://huggingface.co/docs/diffusers/en/api/pipelines/blip_diffusion) and [ref2](https://huggingface.co/docs/diffusers/en/training/custom_diffusion)

"""


"""## Import Packages"""

import json
import itertools
import logging
import math
import os
import random
from pathlib import Path

import accelerate
import numpy as np
import safetensors
import torch
import torch.nn.functional as F
import transformers
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration, set_seed
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms
from tqdm.auto import tqdm
from transformers import AutoTokenizer, CLIPTextModel
from safetensors.torch import load_file

import diffusers
from diffusers.pipelines import BlipDiffusionPipeline
from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, DiffusionPipeline
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import CustomDiffusionAttnProcessor, CustomDiffusionAttnProcessor2_0
from diffusers.optimization import get_scheduler
from diffusers.utils import load_image

"""## Set Random Seed"""

set_seed(42)

"""# Method 1: BLIP Diffusion

## Define Inference Function
"""

def InferenceBlipDiffusion(
    pipe,
    cond_image_path,
    name,
    text_prompt_input,
    guidance_scale,
    num_inference_steps,
    saveDir,
    num_images
):
    """
    Performs inference using a Blip Diffusion pipeline to generate images based on a conditional image and text prompt.

    Args:
        pipe: The Blip Diffusion pipeline object.
        cond_image_path (str): The file path to the conditioning image. This image guides the style and content of the generated images.
        name (str): A descriptive name associated with the subject or concept in the conditioning image. This name is used for both the conditioning and target subjects.
        text_prompt_input (str): The text prompt that provides additional information and guides the generation of the new images.
        guidance_scale (float): A value controlling the influence of the text prompt on the generated images. Higher values enforce the prompt more strongly.
        num_inference_steps (int): The number of denoising steps to perform during the diffusion process. More steps generally lead to higher quality images but take longer to generate.
        saveDir (str): The directory where the generated images will be saved. The function will create this directory if it doesn't exist.
        num_images (int): The number of images to generate.

    Returns:
        None. The generated images are saved to the specified `saveDir`.
    """

    os.makedirs(saveDir, exist_ok = True) # create output directory
    # prepare arguments for BLIP Diffusion
    cond_subject = name
    tgt_subject = name
    cond_image = load_image(cond_image_path)
    negative_prompt = "over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate"

    for i in range(num_images):
        output = pipe(
            text_prompt_input,
            cond_image,
            cond_subject,
            tgt_subject,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            neg_prompt=negative_prompt,
            height=512,
            width=512,
        ).images
        output[0].save(f"{saveDir}/{i}.jpg")

"""## Create BLIP Diffusion Pipeline"""

blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained("Salesforce/blipdiffusion", mean = None, std = None).to("cuda")

"""## Inference"""

with open("ml2025-hw10/metadata.json", "r") as f:
    objects = json.load(f)

##################### TODO: Tune hyperparameters here ##########################

num_inference_steps = 80    # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.
guidance_scale = 10          # Higher guidance scale encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality.

################################################################################

num_images = 15             # The number of images you want to generate.
                            # WARNING: You MUST to keep it 15 if you want to generate the images for submission.
                            # But you can reduce it when tuning hyperparameters to speed up the process
output_dir = "results"

# iterate through each of the 6 objects to customize
for (obj, info) in objects.items():
    # If you only want to generate results for specific object remove the "#" below and adjust the list
    #if (obj not in ["object-1", "object-2", "object-3", "object-4", "object-5", "object-6"]): continue
    InferenceBlipDiffusion(
        pipe = blip_diffusion_pipe,
        cond_image_path = info["path"],
        name = info["name"],
        text_prompt_input = info["text_cond"],
        guidance_scale = guidance_scale,
        num_inference_steps = num_inference_steps,
        saveDir = os.path.join(output_dir, obj),
        num_images = num_images
    )

"""## Archive Results"""

os.system(f"zip -r {output_dir}.zip {output_dir}")      # create zipped file for submission (make sure you generate 15 images for 5 objects)
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-FMdqaABQPpe",
        "jf3qzTRjQUCg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML2025 Homework 10 - Diffusion\n",
        "\n",
        "This notebook is for ML2025 Homework 10, focusing on the customization of diffusion model. The goal is to use BLIP Diffusion and Custom Diffusion to generate images with custom objects.\n",
        "\n",
        "Codes in this notebook is modified from [ref1](https://huggingface.co/docs/diffusers/en/api/pipelines/blip_diffusion) and [ref2](https://huggingface.co/docs/diffusers/en/training/custom_diffusion)\n"
      ],
      "metadata": {
        "id": "yCAjaBfgPnPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilitites"
      ],
      "metadata": {
        "id": "qHDz9O9cQCl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Device"
      ],
      "metadata": {
        "id": "finzPpwQQzIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wlhrT1E_Q04c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ],
      "metadata": {
        "id": "w50IXBsaQ9g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hsichelin/ml2025-hw10.git"
      ],
      "metadata": {
        "id": "UM_b1_CXRAD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "BIXyDO66QcOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import itertools\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import accelerate\n",
        "import numpy as np\n",
        "import safetensors\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import ProjectConfiguration, set_seed\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, CLIPTextModel\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "import diffusers\n",
        "from diffusers.pipelines import BlipDiffusionPipeline\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, DiffusionPipeline\n",
        "from diffusers.loaders import AttnProcsLayers\n",
        "from diffusers.models.attention_processor import CustomDiffusionAttnProcessor, CustomDiffusionAttnProcessor2_0\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.utils import load_image"
      ],
      "metadata": {
        "id": "h1Ysn3pEQ5CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Random Seed"
      ],
      "metadata": {
        "id": "tiLa4LZJT1Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "agRpwvaFT3tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: BLIP Diffusion"
      ],
      "metadata": {
        "id": "-FMdqaABQPpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Inference Function"
      ],
      "metadata": {
        "id": "28juUW0WQipo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def InferenceBlipDiffusion(\n",
        "    pipe,\n",
        "    cond_image_path,\n",
        "    name,\n",
        "    text_prompt_input,\n",
        "    guidance_scale,\n",
        "    num_inference_steps,\n",
        "    saveDir,\n",
        "    num_images\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs inference using a Blip Diffusion pipeline to generate images based on a conditional image and text prompt.\n",
        "\n",
        "    Args:\n",
        "        pipe: The Blip Diffusion pipeline object.\n",
        "        cond_image_path (str): The file path to the conditioning image. This image guides the style and content of the generated images.\n",
        "        name (str): A descriptive name associated with the subject or concept in the conditioning image. This name is used for both the conditioning and target subjects.\n",
        "        text_prompt_input (str): The text prompt that provides additional information and guides the generation of the new images.\n",
        "        guidance_scale (float): A value controlling the influence of the text prompt on the generated images. Higher values enforce the prompt more strongly.\n",
        "        num_inference_steps (int): The number of denoising steps to perform during the diffusion process. More steps generally lead to higher quality images but take longer to generate.\n",
        "        saveDir (str): The directory where the generated images will be saved. The function will create this directory if it doesn't exist.\n",
        "        num_images (int): The number of images to generate.\n",
        "\n",
        "    Returns:\n",
        "        None. The generated images are saved to the specified `saveDir`.\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(saveDir, exist_ok = True) # create output directory\n",
        "    # prepare arguments for BLIP Diffusion\n",
        "    cond_subject = name\n",
        "    tgt_subject = name\n",
        "    cond_image = load_image(cond_image_path)\n",
        "    negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n",
        "\n",
        "    for i in range(num_images):\n",
        "        output = pipe(\n",
        "            text_prompt_input,\n",
        "            cond_image,\n",
        "            cond_subject,\n",
        "            tgt_subject,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            neg_prompt=negative_prompt,\n",
        "            height=512,\n",
        "            width=512,\n",
        "        ).images\n",
        "        output[0].save(f\"{saveDir}/{i}.jpg\")"
      ],
      "metadata": {
        "id": "wd9esgJSUCBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create BLIP Diffusion Pipeline"
      ],
      "metadata": {
        "id": "9Pp1SKZLUGji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained(\"Salesforce/blipdiffusion\", mean = None, std = None).to(\"cuda\")"
      ],
      "metadata": {
        "id": "ZMkGnQfKUKAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "XrpazsMLUNBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ml2025-hw10/metadata.json\", \"r\") as f:\n",
        "    objects = json.load(f)\n",
        "\n",
        "##################### TODO: Tune hyperparameters here ##########################\n",
        "\n",
        "num_inference_steps = 10    # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\n",
        "guidance_scale = 2          # Higher guidance scale encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality.\n",
        "\n",
        "################################################################################\n",
        "\n",
        "num_images = 15             # The number of images you want to generate.\n",
        "                            # WARNING: You MUST to keep it 15 if you want to generate the images for submission.\n",
        "                            # But you can reduce it when tuning hyperparameters to speed up the process\n",
        "output_dir = \"results\"\n",
        "\n",
        "# iterate through each of the 6 objects to customize\n",
        "for (obj, info) in objects.items():\n",
        "    # If you only want to generate results for specific object remove the \"#\" below and adjust the list\n",
        "    #if (obj not in [\"object-1\", \"object-2\", \"object-3\", \"object-4\", \"object-5\", \"object-6\"]): continue\n",
        "    InferenceBlipDiffusion(\n",
        "        pipe = blip_diffusion_pipe,\n",
        "        cond_image_path = info[\"path\"],\n",
        "        name = info[\"name\"],\n",
        "        text_prompt_input = info[\"text_cond\"],\n",
        "        guidance_scale = guidance_scale,\n",
        "        num_inference_steps = num_inference_steps,\n",
        "        saveDir = os.path.join(output_dir, obj),\n",
        "        num_images = num_images\n",
        "    )"
      ],
      "metadata": {
        "id": "nq3fexPiUO8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archive Results"
      ],
      "metadata": {
        "id": "7zxdvPl0VPuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f\"zip -r {output_dir}.zip {output_dir}\")      # create zipped file for submission (make sure you generate 15 images for 5 objects)"
      ],
      "metadata": {
        "id": "hZBACEytVTZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Custom Diffusion"
      ],
      "metadata": {
        "id": "jf3qzTRjQUCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Dataset"
      ],
      "metadata": {
        "id": "R06LX1gfVotL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDiffusionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
        "    It pre-processes the images and the tokenizes prompts.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        concepts_list,\n",
        "        tokenizer,\n",
        "        size=512,\n",
        "        mask_size=64,\n",
        "        center_crop=False,\n",
        "        with_prior_preservation=False,\n",
        "        num_class_images=200,\n",
        "        hflip=False,\n",
        "        aug=True,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.mask_size = mask_size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "        self.interpolation = Image.BILINEAR\n",
        "        self.aug = aug\n",
        "\n",
        "        self.instance_images_path = []\n",
        "        self.class_images_path = []\n",
        "        self.with_prior_preservation = with_prior_preservation\n",
        "        for concept in concepts_list:\n",
        "            inst_img_path = [\n",
        "                (x, concept[\"instance_prompt\"]) for x in Path(concept[\"instance_data_dir\"]).iterdir() if x.is_file()\n",
        "            ]\n",
        "            self.instance_images_path.extend(inst_img_path)\n",
        "\n",
        "            if with_prior_preservation:\n",
        "                class_data_root = Path(concept[\"class_data_dir\"])\n",
        "                if os.path.isdir(class_data_root):\n",
        "                    class_images_path = list(class_data_root.iterdir())\n",
        "                    class_prompt = [concept[\"class_prompt\"] for _ in range(len(class_images_path))]\n",
        "                else:\n",
        "                    with open(class_data_root, \"r\") as f:\n",
        "                        class_images_path = f.read().splitlines()\n",
        "                    with open(concept[\"class_prompt\"], \"r\") as f:\n",
        "                        class_prompt = f.read().splitlines()\n",
        "\n",
        "                class_img_path = list(zip(class_images_path, class_prompt))\n",
        "                self.class_images_path.extend(class_img_path[:num_class_images])\n",
        "\n",
        "        random.shuffle(self.instance_images_path)\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.num_class_images = len(self.class_images_path)\n",
        "        self._length = max(self.num_class_images, self.num_instance_images)\n",
        "        self.flip = transforms.RandomHorizontalFlip(0.5 * hflip)\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                self.flip,\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def preprocess(self, image, scale, resample):\n",
        "        outer, inner = self.size, scale\n",
        "        factor = self.size // self.mask_size\n",
        "        if scale > self.size:\n",
        "            outer, inner = scale, self.size\n",
        "        top, left = np.random.randint(0, outer - inner + 1), np.random.randint(0, outer - inner + 1)\n",
        "        image = image.resize((scale, scale), resample=resample)\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        instance_image = np.zeros((self.size, self.size, 3), dtype=np.float32)\n",
        "        mask = np.zeros((self.size // factor, self.size // factor))\n",
        "        if scale > self.size:\n",
        "            instance_image = image[top : top + inner, left : left + inner, :]\n",
        "            mask = np.ones((self.size // factor, self.size // factor))\n",
        "        else:\n",
        "            instance_image[top : top + inner, left : left + inner, :] = image\n",
        "            mask[\n",
        "                top // factor + 1 : (top + scale) // factor - 1, left // factor + 1 : (left + scale) // factor - 1\n",
        "            ] = 1.0\n",
        "        return instance_image, mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image, instance_prompt = self.instance_images_path[index % self.num_instance_images]\n",
        "        instance_image = Image.open(instance_image)\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        instance_image = self.flip(instance_image)\n",
        "\n",
        "        # apply resize augmentation and create a valid image region mask\n",
        "        random_scale = self.size\n",
        "        if self.aug:\n",
        "            random_scale = (\n",
        "                np.random.randint(self.size // 3, self.size + 1)\n",
        "                if np.random.uniform() < 0.66\n",
        "                else np.random.randint(int(1.2 * self.size), int(1.4 * self.size))\n",
        "            )\n",
        "        instance_image, mask = self.preprocess(instance_image, random_scale, self.interpolation)\n",
        "\n",
        "        if random_scale < 0.6 * self.size:\n",
        "            instance_prompt = np.random.choice([\"a far away \", \"very small \"]) + instance_prompt\n",
        "        elif random_scale > self.size:\n",
        "            instance_prompt = np.random.choice([\"zoomed in \", \"close up \"]) + instance_prompt\n",
        "\n",
        "        example[\"instance_images\"] = torch.from_numpy(instance_image).permute(2, 0, 1)\n",
        "        example[\"mask\"] = torch.from_numpy(mask)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            instance_prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids\n",
        "\n",
        "        if self.with_prior_preservation:\n",
        "            class_image, class_prompt = self.class_images_path[index % self.num_class_images]\n",
        "            class_image = Image.open(class_image)\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_mask\"] = torch.ones_like(example[\"mask\"])\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                class_prompt,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                return_tensors=\"pt\",\n",
        "            ).input_ids\n",
        "\n",
        "        return example"
      ],
      "metadata": {
        "id": "Z7Xo4VmIQbCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Utility Functions"
      ],
      "metadata": {
        "id": "hqD1K8I8Vx34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(examples):\n",
        "    \"\"\"\n",
        "    Puts together a batch of data.\n",
        "\n",
        "    Args:\n",
        "        examples (list of dict): A list of dictionaries, where each dictionary\n",
        "            contains the keys \"instance_prompt_ids\", \"instance_images\", and \"mask\".\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the batched \"input_ids\", \"pixel_values\", and \"mask\".\n",
        "              \"input_ids\" is a concatenated tensor of prompt token IDs.\n",
        "              \"pixel_values\" is a stacked tensor of instance images.\n",
        "              \"mask\" is a stacked tensor of masks with an added channel dimension.\n",
        "    \"\"\"\n",
        "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "    mask = [example[\"mask\"] for example in examples]\n",
        "    input_ids = torch.cat(input_ids, dim=0) # Concatenate prompt token IDs along the batch dimension.\n",
        "    pixel_values = torch.stack(pixel_values) # Stack individual image tensors to form the image batch.\n",
        "    mask = torch.stack(mask) # Stack individual mask tensors to form the mask batch.\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float() # Ensure data is in contiguous memory for potentially faster processing and convert to float.\n",
        "    mask = mask.to(memory_format=torch.contiguous_format).float() # Ensure mask is in contiguous memory and convert to float.\n",
        "\n",
        "    batch = {\"input_ids\": input_ids, \"pixel_values\": pixel_values, \"mask\": mask.unsqueeze(1)} # Add a channel dimension to the mask to be compatible with image processing layers.\n",
        "    return batch\n",
        "\n",
        "def save_new_embed(text_encoder, modifier_token_id, accelerator, modifier_token, output_dir, safe_serialization=True):\n",
        "    \"\"\"Saves the new token embeddings learned by the text encoder.\n",
        "\n",
        "    Args:\n",
        "        text_encoder: The trained text encoder model.\n",
        "        modifier_token_id (list of int): List of token IDs corresponding to the modifier tokens.\n",
        "        accelerator: The accelerator object used for distributed training.\n",
        "        modifier_token (list of str): List of the modifier tokens (e.g., [\"<new1>\"]).\n",
        "        output_dir (str): The directory where the new embeddings will be saved.\n",
        "        safe_serialization (bool, optional): Whether to use safe serialization (safetensors). Defaults to True.\n",
        "    \"\"\"\n",
        "    # Unwrap the potentially distributed text encoder model to access its components.\n",
        "    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight\n",
        "\n",
        "    # Iterate through the modifier tokens and their corresponding IDs.\n",
        "    for x, y in zip(modifier_token_id, [modifier_token]):\n",
        "        # Create a dictionary to store the learned embedding for the current modifier token.\n",
        "        learned_embeds_dict = {}\n",
        "        # Extract the learned embedding for the specific modifier token ID.\n",
        "        learned_embeds_dict[y] = learned_embeds[x]\n",
        "\n",
        "        # Determine the filename and saving method based on the safe_serialization flag.\n",
        "        if safe_serialization:\n",
        "            filename = f\"{output_dir}/{y}.safetensors\"\n",
        "            # Save the learned embedding dictionary using the safetensors format.\n",
        "            safetensors.torch.save_file(learned_embeds_dict, filename, metadata={\"format\": \"pt\"})\n",
        "        else:\n",
        "            filename = f\"{output_dir}/{y}.bin\"\n",
        "            # Save the learned embedding dictionary using the standard PyTorch format.\n",
        "            torch.save(learned_embeds_dict, filename)"
      ],
      "metadata": {
        "id": "GKtCY6xcV9ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ],
      "metadata": {
        "id": "2_qe8NscWFTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func(\n",
        "    output_dir,\n",
        "    instance_prompt,\n",
        "    instance_data_dir,\n",
        "    freeze_model,\n",
        "    learning_rate,\n",
        "    max_train_steps,\n",
        "    train_batch_size\n",
        "):\n",
        "    accelerator_project_config = ProjectConfiguration(project_dir=output_dir, logging_dir=Path(output_dir, \"logs\"))\n",
        "\n",
        "    # create accelerator\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=1,\n",
        "        mixed_precision=None,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_config=accelerator_project_config,\n",
        "    )\n",
        "\n",
        "    # Disable AMP for MPS.\n",
        "    if torch.backends.mps.is_available():\n",
        "        accelerator.native_amp = False\n",
        "\n",
        "    # We need to initialize the trackers we use, and also store our configuration.\n",
        "    # The trackers initializes automatically on the main process.\n",
        "    if accelerator.is_main_process:\n",
        "        accelerator.init_trackers(\"custom-diffusion\")\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if accelerator.is_main_process and (output_dir is not None):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "    # create tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\",\n",
        "        revision=None,\n",
        "        use_fast=False,\n",
        "    )\n",
        "\n",
        "    # Load scheduler and models\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=None, variant=None\n",
        "    )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"vae\", revision=None, variant=None\n",
        "    )\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"unet\", revision=None, variant=None\n",
        "    )\n",
        "\n",
        "    # Adding a modifier token which is optimized ####\n",
        "    modifier_token_id = []\n",
        "    initializer_token_id = []\n",
        "    modifier_token = \"<new1>\"\n",
        "    initializer_token = \"ktn\"\n",
        "\n",
        "    # Add the placeholder token in tokenizer\n",
        "    num_added_tokens = tokenizer.add_tokens(modifier_token)\n",
        "    if num_added_tokens == 0:\n",
        "        raise ValueError(\n",
        "            f\"The tokenizer already contains the token {modifier_token}. Please pass a different\"\n",
        "            \" `modifier_token` that is not already in the tokenizer.\"\n",
        "        )\n",
        "\n",
        "    # Convert the initializer_token, placeholder_token to ids\n",
        "    token_ids = tokenizer.encode([initializer_token], add_special_tokens=False)\n",
        "\n",
        "    # Check if initializer_token is a single token or a sequence of tokens\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(\"The initializer token must be a single token.\")\n",
        "\n",
        "    initializer_token_id.append(token_ids[0])\n",
        "    modifier_token_id.append(tokenizer.convert_tokens_to_ids(modifier_token))\n",
        "\n",
        "    # Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
        "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Initialise the newly added placeholder token with the embeddings of the initializer token\n",
        "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
        "    for x, y in zip(modifier_token_id, initializer_token_id):\n",
        "        token_embeds[x] = token_embeds[y]\n",
        "\n",
        "    # Freeze all parameters except for the token embeddings in text encoder\n",
        "    params_to_freeze = itertools.chain(\n",
        "        text_encoder.text_model.encoder.parameters(),\n",
        "        text_encoder.text_model.final_layer_norm.parameters(),\n",
        "        text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
        "    )\n",
        "    for param in params_to_freeze:\n",
        "        param.requires_grad = False\n",
        "\n",
        "    vae.requires_grad_(False)\n",
        "    if modifier_token is None:\n",
        "        text_encoder.requires_grad_(False)\n",
        "    unet.requires_grad_(False)\n",
        "\n",
        "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
        "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Move unet, vae and text_encoder to device and cast to weight_dtype\n",
        "    if accelerator.mixed_precision != \"fp16\" and modifier_token is not None:\n",
        "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "    unet.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    attention_class = (\n",
        "        CustomDiffusionAttnProcessor2_0 if hasattr(F, \"scaled_dot_product_attention\") else CustomDiffusionAttnProcessor\n",
        "    )\n",
        "\n",
        "    # now we will add new Custom Diffusion weights to the attention layers\n",
        "    # It's important to realize here how many attention weights will be added and of which sizes\n",
        "    # The sizes of the attention layers consist only of two different variables:\n",
        "    # 1) - the \"hidden_size\", which is increased according to `unet.config.block_out_channels`.\n",
        "    # 2) - the \"cross attention size\", which is set to `unet.config.cross_attention_dim`.\n",
        "\n",
        "    # Let's first see how many attention processors we will have to set.\n",
        "    # For Stable Diffusion, it should be equal to:\n",
        "    # - down blocks (2x attention layers) * (2x transformer layers) * (3x down blocks) = 12\n",
        "    # - mid blocks (2x attention layers) * (1x transformer layers) * (1x mid blocks) = 2\n",
        "    # - up blocks (2x attention layers) * (3x transformer layers) * (3x down blocks) = 18\n",
        "    # => 32 layers\n",
        "\n",
        "    # Only train key, value projection layers if freeze_model = 'crossattn_kv' else train all params in the cross attention layer\n",
        "    train_kv = True\n",
        "    train_q_out = False if freeze_model == \"crossattn_kv\" else True\n",
        "    custom_diffusion_attn_procs = {}\n",
        "\n",
        "    st = unet.state_dict()\n",
        "    for name, _ in unet.attn_processors.items():\n",
        "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "        if name.startswith(\"mid_block\"):\n",
        "            hidden_size = unet.config.block_out_channels[-1]\n",
        "        elif name.startswith(\"up_blocks\"):\n",
        "            block_id = int(name[len(\"up_blocks.\")])\n",
        "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
        "        elif name.startswith(\"down_blocks\"):\n",
        "            block_id = int(name[len(\"down_blocks.\")])\n",
        "            hidden_size = unet.config.block_out_channels[block_id]\n",
        "        layer_name = name.split(\".processor\")[0]\n",
        "        weights = {\n",
        "            \"to_k_custom_diffusion.weight\": st[layer_name + \".to_k.weight\"],\n",
        "            \"to_v_custom_diffusion.weight\": st[layer_name + \".to_v.weight\"],\n",
        "        }\n",
        "        if train_q_out:\n",
        "            weights[\"to_q_custom_diffusion.weight\"] = st[layer_name + \".to_q.weight\"]\n",
        "            weights[\"to_out_custom_diffusion.0.weight\"] = st[layer_name + \".to_out.0.weight\"]\n",
        "            weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n",
        "        if cross_attention_dim is not None:\n",
        "            custom_diffusion_attn_procs[name] = attention_class(\n",
        "                train_kv=train_kv,\n",
        "                train_q_out=train_q_out,\n",
        "                hidden_size=hidden_size,\n",
        "                cross_attention_dim=cross_attention_dim,\n",
        "            ).to(unet.device)\n",
        "            custom_diffusion_attn_procs[name].load_state_dict(weights)\n",
        "        else:\n",
        "            custom_diffusion_attn_procs[name] = attention_class(\n",
        "                train_kv=False,\n",
        "                train_q_out=False,\n",
        "                hidden_size=hidden_size,\n",
        "                cross_attention_dim=cross_attention_dim,\n",
        "            )\n",
        "    del st\n",
        "    unet.set_attn_processor(custom_diffusion_attn_procs)\n",
        "    custom_diffusion_layers = AttnProcsLayers(unet.attn_processors)\n",
        "    accelerator.register_for_checkpointing(custom_diffusion_layers)\n",
        "\n",
        "    # rescale learning rate\n",
        "    learning_rate = learning_rate * train_batch_size * accelerator.num_processes\n",
        "\n",
        "    # Optimizer creation\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        itertools.chain(text_encoder.get_input_embeddings().parameters(), custom_diffusion_layers.parameters())\n",
        "        if modifier_token is not None\n",
        "        else custom_diffusion_layers.parameters(),\n",
        "        lr=learning_rate,\n",
        "        betas=(0.9, 0.999),\n",
        "        weight_decay=1e-2,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Dataset and DataLoaders creation:\n",
        "    concepts_list = [\n",
        "        {\n",
        "            \"instance_prompt\": instance_prompt,\n",
        "            \"class_prompt\": None,\n",
        "            \"instance_data_dir\": instance_data_dir,\n",
        "            \"class_data_dir\": None,\n",
        "        }\n",
        "    ]\n",
        "    resolution = 512\n",
        "\n",
        "    train_dataset = CustomDiffusionDataset(\n",
        "        concepts_list=concepts_list,\n",
        "        tokenizer=tokenizer,\n",
        "        size=resolution,\n",
        "        mask_size=vae.encode(\n",
        "            torch.randn(1, 3, resolution, resolution).to(dtype=weight_dtype).to(accelerator.device)\n",
        "        ).latent_dist.sample().size()[-1],\n",
        "        center_crop=False,\n",
        "        hflip=True,\n",
        "        aug=False,\n",
        "    )\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda examples: collate_fn(examples),\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation.\n",
        "    num_warmup_steps_for_scheduler = 0\n",
        "    num_training_steps_for_scheduler = max_train_steps * accelerator.num_processes\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"constant\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps_for_scheduler,\n",
        "        num_training_steps=num_training_steps_for_scheduler,\n",
        "    )\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    if modifier_token is not None:\n",
        "        custom_diffusion_layers, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            custom_diffusion_layers, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "    else:\n",
        "        custom_diffusion_layers, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            custom_diffusion_layers, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = len(train_dataloader)\n",
        "    # Afterwards we recalculate our number of training epochs\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # Train!\n",
        "    global_step = 0\n",
        "    first_epoch = 0\n",
        "    initial_global_step = 0\n",
        "\n",
        "    # progress bar\n",
        "    progress_bar = tqdm(\n",
        "        range(0, max_train_steps),\n",
        "        initial=initial_global_step,\n",
        "        desc=\"Steps\",\n",
        "        # Only show the progress bar once on each machine.\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "    )\n",
        "\n",
        "    for epoch in range(first_epoch, num_train_epochs):\n",
        "        unet.train()\n",
        "        if modifier_token is not None:\n",
        "            text_encoder.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet), accelerator.accumulate(text_encoder):\n",
        "                # Convert images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                # Get the target for loss depending on the prediction type\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                mask = batch[\"mask\"]\n",
        "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
        "                loss = ((loss * mask).sum([1, 2, 3]) / mask.sum([1, 2, 3])).mean()\n",
        "                accelerator.backward(loss)\n",
        "                # Zero out the gradients for all token embeddings except the newly added\n",
        "                # embeddings for the concept, as we only want to optimize the concept embeddings\n",
        "                if modifier_token is not None:\n",
        "                    if accelerator.num_processes > 1:\n",
        "                        grads_text_encoder = text_encoder.module.get_input_embeddings().weight.grad\n",
        "                    else:\n",
        "                        grads_text_encoder = text_encoder.get_input_embeddings().weight.grad\n",
        "                    # Get the index for tokens that we want to zero the grads for\n",
        "                    index_grads_to_zero = torch.arange(len(tokenizer)) != modifier_token_id[0]\n",
        "                    for i in range(1, len(modifier_token_id)):\n",
        "                        index_grads_to_zero = index_grads_to_zero & (\n",
        "                            torch.arange(len(tokenizer)) != modifier_token_id[i]\n",
        "                        )\n",
        "                    grads_text_encoder.data[index_grads_to_zero, :] = grads_text_encoder.data[\n",
        "                        index_grads_to_zero, :\n",
        "                    ].fill_(0)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    params_to_clip = (\n",
        "                        itertools.chain(text_encoder.parameters(), custom_diffusion_layers.parameters())\n",
        "                        if modifier_token is not None\n",
        "                        else custom_diffusion_layers.parameters()\n",
        "                    )\n",
        "                    accelerator.clip_grad_norm_(params_to_clip, 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad(set_to_none=False)\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if accelerator.is_main_process and (global_step == max_train_steps):\n",
        "                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
        "                    accelerator.save_state(save_path)\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "\n",
        "            if global_step >= max_train_steps:\n",
        "                break\n",
        "\n",
        "    # Save the custom diffusion layers\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        unet = unet.to(torch.float32)\n",
        "        unet.save_attn_procs(output_dir, safe_serialization=True)\n",
        "        save_new_embed(\n",
        "            text_encoder,\n",
        "            modifier_token_id,\n",
        "            accelerator,\n",
        "            modifier_token,\n",
        "            output_dir,\n",
        "            safe_serialization=True,\n",
        "        )\n",
        "\n",
        "    accelerator.end_training()"
      ],
      "metadata": {
        "id": "iTZufl9qWIU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "W1g1qMK4WNGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### TODO: Tune hyperparameters here ##########################\n",
        "\n",
        "instance_prompt = \"photo of a <new1> toy\"           # The text prompt used for training\n",
        "instance_data_dir = \"ml2025-hw10/data/object-5\"     # Path to images of the object to customize\n",
        "parameter_to_train = \"crossattn_kv\"                 # \"crossattn_kv\" only train the K V in cross attention. Change this to \"crossattn\" if you also want to train Q\n",
        "learning_rate = 2e-5\n",
        "max_train_steps = 100\n",
        "train_batch_size = 2\n",
        "\n",
        "################################################################################\n",
        "\n",
        "ckpt_dir = \"output\"                               # directory name to save checkpoints"
      ],
      "metadata": {
        "id": "9O8NyqKqWO3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "jpnK349mWSdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accelerate.notebook_launcher(train_func, args=(ckpt_dir, instance_prompt, instance_data_dir, parameter_to_train, learning_rate, max_train_steps, train_batch_size))"
      ],
      "metadata": {
        "id": "ICk8w4vOWTzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Fine-tuned Model"
      ],
      "metadata": {
        "id": "2KIZx86vWV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "state_dict = load_file(os.path.join(ckpt_dir, \"pytorch_custom_diffusion_weights.safetensors\"), device = \"cpu\")\n",
        "custom_attn = pipe.unet._process_custom_diffusion(state_dict=state_dict)\n",
        "attn_procs = pipe.unet.attn_processors\n",
        "attn_procs.update(custom_attn)\n",
        "pipe.unet.set_attn_processor(attn_procs)\n",
        "pipe.unet.to(dtype = pipe.unet.dtype, device = pipe.unet.device)\n",
        "\n",
        "pipe.load_textual_inversion(ckpt_dir, weight_name=\"<new1>.safetensors\")"
      ],
      "metadata": {
        "id": "N8R7siyLWZ0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "wGgUfU9QWttS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### TODO: Tune hyperparameters here ##########################\n",
        "\n",
        "generate_prompt = \"a <new1> toy in the snow\"    # text prompt to generate images (make sure the condition is correct if you are not customizing for object5)\n",
        "                                                # WARNING: if you modified \"instance_prompt\" for training you would like to modify this correspondingly\n",
        "                                                #          e.g., If your instanace_prompt is \"photo of a <new1> structure\" you would like to use \"a <new1> structure in the snow\"\n",
        "\n",
        "num_inference_steps = 100   # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\n",
        "guidance_scale = 6.0        # Higher guidance scale encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality.\n",
        "\n",
        "################################################################################\n",
        "\n",
        "obj = instance_data_dir.split(\"/\")[-1]\n",
        "output_dir = \"results\"\n",
        "os.makedirs(f\"{output_dir}/{obj}\", exist_ok = True)\n",
        "\n",
        "for i in range(15):\n",
        "    image = pipe(\n",
        "        generate_prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        eta=1.0,\n",
        "    ).images[0]\n",
        "    image.save(f\"{output_dir}/{obj}/{i}.jpg\")"
      ],
      "metadata": {
        "id": "J_tOtf40Wwex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archive Results"
      ],
      "metadata": {
        "id": "_H5GE90cXBdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f\"zip -r {output_dir}.zip {output_dir}\")      # create zipped file for submission (make sure you generate 15 images for 5 objects)"
      ],
      "metadata": {
        "id": "Pm2wrY1tXBFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
# -*- coding: utf-8 -*-
"""Homework5_Finetuning_is_Powerful.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XTLX9o2QveOs71-njzw2ZLNcz7RIDhyX


"""
"""
### Unsloth

#### Note : Changing the model is against the Rules of Homework 5 !!!

### Initialize the LLM
"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

### Changing the model here is forbidden !

# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name = "unsloth/Llama-2-7b-bnb-4bit",    ### Do not change the model for any other models or quantization versions
#     max_seq_length = max_seq_length,
#     dtype = dtype,
#     load_in_4bit = load_in_4bit,
#     # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
# )

# 檢查本地路徑是否存在已下載的模型
import os
from pathlib import Path
local_model_path = "./llama2-7b-model"
model_file_path = Path(local_model_path) / "model.safetensors"

if model_file_path.exists():
    print(f"找到本地模型，使用路徑: {local_model_path}")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = local_model_path,  # 使用本地路徑
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
else:
    print("未找到本地模型，從 Hugging Face 下載...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "unsloth/Llama-2-7b-bnb-4bit",  # 從 Hugging Face 下載
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )

"""Add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################

model = FastLanguageModel.get_peft_model(
    model,
    r = 96, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.
    lora_alpha = 192,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128


################# TODO  ####################################################################
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",  ### Use llama-3.1 template for better performance here
)

def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }
pass

"""# Dataset Preperation (Loading and Refining)

## Data Filtering & Sorting
"""

from datasets import load_dataset, Dataset, load_from_disk

# Load the dataset from Hugging Face
dataset = load_from_disk("./ML_Spring2025_HW5/fastchat_alpaca_52k")

# ---------------------------
# Add a "text" field to each example
# ---------------------------
# This function extracts the first assistant message from the conversation
def add_text_field(example):
    # Extract the first message where role == 'assistant'
    assistant_texts = [msg["content"] for msg in example["conversations"] if msg["role"] == "assistant"]
    text = assistant_texts[0] if assistant_texts else ""
    return {"text": text}

# Map the function over the dataset to add the "text" column.
dataset = dataset.map(add_text_field)

# Print the dataset structure to confirm the new feature.
print(dataset)


# ---------------------------
#################### TODO : Define a helper function for computing conversation length ###############

# The default "conversation length" here refers to the length of the input (human) and output (gpt), you can modify it at your will

def compute_conversation_length(example):
    # Compute total word count across all messages in the 'conversations' field
    return sum(len(message["content"].split()) for message in example["conversations"])


#################### TODO ############################################################################

# ---------------------------
# Simple Sorting Method  (Default)
# ---------------------------
# Sort the dataset from shortest to longest conversation (by word count)
sorted_dataset_simple_list = sorted(dataset, key=compute_conversation_length, reverse=False)

# Convert back to a Dataset object
sorted_dataset_simple = Dataset.from_list(sorted_dataset_simple_list)

print("\nTop examples sorted by simple conversation length:")
for entry in sorted_dataset_simple.select(range(5)):
    print(f"ID: {entry['id']}, Conversation Length: {compute_conversation_length(entry)}")
# ---------------------------

# 在計算對話長度的函數之後添加以下代碼
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde
import os

# 全局變量儲存分析結果
dataset_analysis_results = {
    "ideal_length_min": 100,
    "ideal_length_max": 300,
    "length_weight": 0.3,
    "score_weight": 0.7,
    "visualization_done": False
}

def analyze_dataset_distribution(dataset, save_plots=True):
    """
    分析資料集的分布並繪製視覺化圖表
    返回最佳的對話長度範圍和權重建議
    """
    global dataset_analysis_results
    
    if dataset_analysis_results["visualization_done"]:
        return dataset_analysis_results
    
    # 收集資料集中所有項目的對話長度和分數
    conversation_lengths = [compute_conversation_length(example) for example in dataset]
    scores = [example["score"] for example in dataset]
    
    # 計算基本統計資訊
    avg_length = np.mean(conversation_lengths)
    median_length = np.median(conversation_lengths)
    avg_score = np.mean(scores)
    
    # 找出長度-分數的相關性
    correlation = np.corrcoef(conversation_lengths, scores)[0, 1]
    
    # 找出分數最高的範例的長度分布
    top_scores_indices = np.argsort(scores)[-int(len(scores)*0.1):]  # 取分數最高的10%
    top_lengths = [conversation_lengths[i] for i in top_scores_indices]
    
    # 確定理想長度範圍
    ideal_length_min = max(50, np.percentile(top_lengths, 25))  # 25%分位數
    ideal_length_max = min(500, np.percentile(top_lengths, 75))  # 75%分位數
    
    if save_plots:
        # 確保輸出目錄存在
        os.makedirs("plots", exist_ok=True)
        
        # 修改繪圖部分的標題為英文
        # 1. 繪製散點圖
        plt.figure(figsize=(12, 8))
        plt.scatter(conversation_lengths, scores, alpha=0.3, s=5)
        plt.title('Conversation Length vs Score Distribution')
        plt.xlabel('Conversation Length (words)')
        plt.ylabel('Score')
        plt.axvline(x=ideal_length_min, color='r', linestyle='--', label=f'Ideal Min: {ideal_length_min:.0f}')
        plt.axvline(x=ideal_length_max, color='g', linestyle='--', label=f'Ideal Max: {ideal_length_max:.0f}')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.savefig('plots/length_score_scatter.png')

        # 2. 繪製密度圖
        x = np.array(conversation_lengths)
        y = np.array(scores)
        # 過濾掉極端值以便更好地可視化
        mask = (x < np.percentile(x, 99))
        x_filtered = x[mask]
        y_filtered = y[mask]

        # 只有在有足夠數據點時才計算密度
        if len(x_filtered) > 10:
            try:
                xy = np.vstack([x_filtered, y_filtered])
                density = gaussian_kde(xy)(xy)
                
                plt.figure(figsize=(12, 8))
                plt.scatter(x_filtered, y_filtered, c=density, s=5, cmap='viridis')
                plt.colorbar(label='Density')
                plt.title('Density Distribution of Length vs Score')
                plt.xlabel('Conversation Length (words)')
                plt.ylabel('Score')
                plt.axvline(x=ideal_length_min, color='r', linestyle='--', label=f'Ideal Min: {ideal_length_min:.0f}')
                plt.axvline(x=ideal_length_max, color='g', linestyle='--', label=f'Ideal Max: {ideal_length_max:.0f}')
                plt.grid(True, linestyle='--', alpha=0.7)
                plt.legend()
                plt.savefig('plots/density_plot.png')
            except:
                print("Density plot failed, possibly due to insufficient data points or unsuitable distribution")

        # 3. 繪製對話長度直方圖
        plt.figure(figsize=(12, 6))
        plt.hist(conversation_lengths, bins=50, alpha=0.7)
        plt.title('Conversation Length Histogram')
        plt.xlabel('Conversation Length (words)')
        plt.ylabel('Frequency')
        plt.axvline(x=ideal_length_min, color='r', linestyle='--', label=f'Ideal Min: {ideal_length_min:.0f}')
        plt.axvline(x=ideal_length_max, color='g', linestyle='--', label=f'Ideal Max: {ideal_length_max:.0f}')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.savefig('plots/length_histogram.png')

        # 4. 繪製分數直方圖
        plt.figure(figsize=(12, 6))
        plt.hist(scores, bins=50, alpha=0.7)
        plt.title('Score Distribution Histogram')
        plt.xlabel('Score')
        plt.ylabel('Frequency')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.savefig('plots/score_histogram.png')

        plt.close('all')  # 關閉所有圖表以釋放記憶體
    
    # 根據相關性調整權重
    length_weight = 0.3
    score_weight = 0.7
    
    # 如果長度與分數有較強相關性，調整權重
    if abs(correlation) > 0.3:
        if correlation > 0:  # 正相關，長文本傾向於高分
            length_weight = 0.4
            score_weight = 0.6
        else:  # 負相關，短文本傾向於高分
            length_weight = 0.2
            score_weight = 0.8
    
    # 存儲分析結果
    dataset_analysis_results = {
        "ideal_length_min": ideal_length_min,
        "ideal_length_max": ideal_length_max,
        "length_weight": length_weight,
        "score_weight": score_weight,
        "avg_length": avg_length,
        "median_length": median_length,
        "avg_score": avg_score,
        "correlation": correlation,
        "visualization_done": True
    }
    
    # 輸出分析結果
    print(f"\n===== Dataset Analysis Results =====")
    print(f"Ideal conversation length range: {ideal_length_min:.0f} - {ideal_length_max:.0f} words")
    print(f"Length-Score correlation: {correlation:.3f}")
    print(f"Recommended weights: Length {length_weight:.1f}, Score {score_weight:.1f}")
    print(f"Average conversation length: {avg_length:.1f}, Median: {median_length:.0f}")
    print(f"Average score: {avg_score:.3f}")
    print("=======================\n")
    
    return dataset_analysis_results


############## Advanced Sorting Method (TODO : Modify the sorting key) ##################
# ---------------------------
# 基於數據分析的高級排序方法
def advanced_sort_key(example):
    # 先確保分析已完成
    analysis = analyze_dataset_distribution(dataset, save_plots=True)
    
    conversation_len = compute_conversation_length(example)
    score = example["score"]
    
    # 從分析結果獲取理想長度範圍
    ideal_length_min = analysis["ideal_length_min"]
    ideal_length_max = analysis["ideal_length_max"]
    
    # 計算長度得分 (0-1範圍)
    if conversation_len < ideal_length_min:
        length_score = conversation_len / ideal_length_min  # 對短對話進行線性懲罰
    elif conversation_len > ideal_length_max:
        length_score = max(0, 1 - (conversation_len - ideal_length_max) / (ideal_length_max * 0.5))  # 對長對話進行較緩慢懲罰
    else:
        length_score = 1.0  # 在理想範圍內，給予最高分
    
    # 從分析結果獲取權重
    length_weight = analysis["length_weight"]
    score_weight = analysis["score_weight"]
    
    # 組合長度得分與原始分數
    final_score = score * score_weight + length_score * length_weight
    
    return final_score

####################################### TODO ###########################################

sorted_dataset_advanced_list = sorted(dataset, key=advanced_sort_key, reverse=True)
# Convert back to a Dataset object
sorted_dataset_advanced = Dataset.from_list(sorted_dataset_advanced_list)

# 1. 精選更高質量數據 (60%)
high_quality = sorted_dataset_advanced.select(range(0,60))

# 2. 增加複雜推理數據比例 (20%)
hard_samples = sorted_dataset_advanced.select(range(0,1000)).filter(
    lambda x: ("explain" in " ".join([m["content"] for m in x["conversations"]]) or 
              "why" in " ".join([m["content"] for m in x["conversations"]]) or
              "how" in " ".join([m["content"] for m in x["conversations"]]) or
              "analysis" in " ".join([m["content"] for m in x["conversations"]]) or
              "complex" in " ".join([m["content"] for m in x["conversations"]])) and
              x["score"] > 0.75  # 提高質量門檻
).select(range(20))

# 3. 增加多樣性數據 (20%)
diverse_samples = sorted_dataset_advanced.select(range(100,800)).filter(
    lambda x: compute_conversation_length(x) < dataset_analysis_results["ideal_length_max"] * 1.2 and 
              compute_conversation_length(x) > dataset_analysis_results["ideal_length_min"] * 0.8 and
              x["score"] > 0.65
).select(range(20))

# print("\nTop examples sorted by advanced key (combination of conversation length and score):")
# for entry in sorted_dataset_advanced.select(range(5)):
#     print(f"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}")

"""#### Note : You are limited to use 100 sorted data among the 1000 data in the given dataset, no more than 100 data is allowed for training !!!"""

################# TODO : select the simple or advanced dataset for training ##############

# dataset_used = "sorted_dataset_simple"
dataset_used = "sorted_dataset_advanced"

################# TODO ###################################################################

if dataset_used == "sorted_dataset_simple":
    train_dataset = sorted_dataset_simple.select(range(0,100))    ### You can also select from the middle, e.g. sorted_dataset_simple.select(range(50,150))
else:
    from datasets import concatenate_datasets
    train_dataset = concatenate_datasets([high_quality, hard_samples, diverse_samples])

from unsloth.chat_templates import standardize_sharegpt
train_dataset = standardize_sharegpt(train_dataset)
train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)

"""# Dataset Visualize"""

dataset[5]["conversations"]

"""And we see how the chat template transformed these conversations.

**[Notice]** Llama 3.1 Instruct's default chat template default adds `"Cutting Knowledge Date: December 2023\nToday Date: 26 July 2024"`, so do not be alarmed!
"""

dataset[5]["text"]

"""## Training"""

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported


################# TODO : Tweak the training hyperparameters here.  #####################


training_config = {
    "per_device_train_batch_size": 1,       # 減小批次進一步增加更新次數
    "gradient_accumulation_steps": 16,      # 大幅增加累積步驟，提高有效批次大小
    "warmup_ratio": 0.1,                    # 使用比例式預熱而非固定步數
    "num_train_epochs": 8,                  # 大幅增加訓練輪數
    "learning_rate": 8e-5,                  # 開始時使用較大學習率
    "optim": "adamw_8bit",
    "weight_decay": 0.05,                   # 增加正則化
    "lr_scheduler_type": "cosine_with_restarts", 
    "max_grad_norm": 0.3,                   # 梯度裁剪，避免不穩定
    "seed": 3407,
}


################# TODO #################################################################

# """We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."""

from unsloth.chat_templates import train_on_responses_only

# """#### TODO : Curriculum Training  (Optional)
# start training the LLM with “easier” examples (e.g., shorter, clearer conversations) and progressively introduce more complex ones.

# The total data amount used to train should still not exceed 100 data.
# """

# ############## TODO : Curriculum Training  ######################

# 第一階段：高學習率快速收斂
trainer_1 = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=16,
        warmup_ratio=0.1,
        num_train_epochs=4,
        learning_rate=1e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.05,
        lr_scheduler_type="cosine_with_restarts",
        max_grad_norm=0.3,
        seed=3407,
        output_dir="outputs/stage1",
        report_to="none",
    ),
)
trainer_1 = train_on_responses_only(trainer_1, instruction_part="<|start_header_id|>user<|end_header_id|>\n\n", 
                                   response_part="<|start_header_id|>assistant<|end_header_id|>\n\n")
trainer_1.train()

# 第二階段：低學習率微調
trainer_2 = SFTTrainer(
    model=model,  # 使用上階段訓練的模型
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=16,
        warmup_ratio=0.05,
        num_train_epochs=3,
        learning_rate=2e-5,  # 極低學習率
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.03,
        lr_scheduler_type="cosine",  # simple cosine decay
        max_grad_norm=0.3,
        seed=3407,
        output_dir="outputs/stage2",
        report_to="none",
    ),
)
trainer_2 = train_on_responses_only(trainer_2, instruction_part="<|start_header_id|>user<|end_header_id|>\n\n", 
                                   response_part="<|start_header_id|>assistant<|end_header_id|>\n\n")
trainer_2.train()

# 第三階段：低學習率微調
trainer_3 = SFTTrainer(
    model=model,  # 使用上階段訓練的模型
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=16,
        warmup_ratio=0.05,
        num_train_epochs=1,
        learning_rate=5e-6,  # 極低學習率
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.03,
        lr_scheduler_type="constant",  # 恆定學習率
        max_grad_norm=0.3,
        seed=3407,
        output_dir="outputs/stage2",
        report_to="none",
    ),
)
trainer_3 = train_on_responses_only(trainer_3, instruction_part="<|start_header_id|>user<|end_header_id|>\n\n", 
                                   response_part="<|start_header_id|>assistant<|end_header_id|>\n\n")
trainer_3.train()

"""<a name="Inference"></a>
## Inference

"""

def parse_true_output(text):
    """
    Extracts the true assistant output from the decoded model output.

    It looks for the assistant header token:
        "<|start_header_id|>assistant<|end_header_id|>\n\n"
    and extracts everything after it until the first occurrence of "<|eot_id|>".
    If the assistant header is not found, it falls back to the last occurrence
    of "<|end_header_id|>\n\n". If "<|eot_id|>" is not found, the extraction
    continues until the end of the string.
    """
    assistant_header = "<|start_header_id|>assistant<|end_header_id|>\n\n"
    start_index = text.find(assistant_header)
    if start_index != -1:
        start_index += len(assistant_header)
    else:
        # Fallback: use the last occurrence of the generic header ending
        generic_header = "<|end_header_id|>\n\n"
        start_index = text.rfind(generic_header)
        if start_index != -1:
            start_index += len(generic_header)
        else:
            start_index = 0

    end_index = text.find("<|eot_id|>", start_index)
    if end_index == -1:
        end_index = len(text)
    return text[start_index:end_index].strip()

from unsloth.chat_templates import get_chat_template
import json
from datetime import datetime

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

# Load the test set JSON file (without GPT responses)
with open("./ML_Spring2025_HW5/test_set_evol_instruct_150.json", "r") as infile:
    test_data = json.load(infile)

# Dictionary to store inference results
inference_results = {}

# 更先進的動態生成參數
def get_dynamic_generation_params(messages):
    # 獲取用戶輸入文本以進行分析
    user_inputs = " ".join([m["content"] for m in messages if m["role"] == "user"])
    
    # 分析用戶請求類型
    is_code_request = any(kw in user_inputs.lower() for kw in ["code", "program", "function", "algorithm", "implement"])
    is_brief_explanation = any(kw in user_inputs.lower() for kw in ["explain briefly", "summary", "tldr", "short description"])
    is_detailed_explanation = any(kw in user_inputs.lower() for kw in ["explain in detail", "thorough", "comprehensive", "step by step"]) 
    is_creative = any(kw in user_inputs.lower() for kw in ["story", "creative", "imagine", "design", "generate"])
    is_factual = any(kw in user_inputs.lower() for kw in ["fact", "information", "data", "statistics"])
    
    # 代碼生成參數
    if is_code_request:
        return {
            "do_sample": False,              # 代碼生成需要確定性
            "max_new_tokens": 500,           # 代碼可能需要更多的token
            "temperature": 0.1,              # 低溫度確保更確定性的輸出
            "top_p": 0.9,                    # 稍微縮小範圍但仍允許一些變化
            "top_k": 20,                     # 限制更少的選項以提高質量
            "repetition_penalty": 1.2        # 避免重複的代碼段
        }
    # 簡短說明參數
    elif is_brief_explanation:
        return {
            "do_sample": True,
            "max_new_tokens": 150,           # 限制回覆長度
            "temperature": 0.7,              # 中等溫度
            "top_p": 0.85,                   # 確保簡潔的關鍵信息
            "top_k": 30,
            "repetition_penalty": 1.1        # 輕微懲罰重複
        }
    # 詳細解釋參數
    elif is_detailed_explanation:
        return {
            "do_sample": True,
            "max_new_tokens": 800,           # 允許更長的回覆
            "temperature": 0.75,             # 中等溫度以保持一致性
            "top_p": 0.92,                   # 廣泛覆蓋主題
            "top_k": 50,                     # 允許更多選擇
            "repetition_penalty": 1.05       # 輕微懲罰重複
        }
    # 創意內容參數
    elif is_creative:
        return {
            "do_sample": True,
            "max_new_tokens": 600,           # 創意內容通常較長
            "temperature": 0.9,              # 高溫度鼓勵創意
            "top_p": 0.95,                   # 更多變化
            "top_k": 80,                     # 允許更多的字詞選擇
            "repetition_penalty": 1.03       # 很少的重複懲罰
        }
    # 事實性內容參數
    elif is_factual:
        return {
            "do_sample": False,              # 事實內容需要確定性
            "max_new_tokens": 350,           # 中等長度
            "temperature": 0.3,              # 低溫度以確保一致性
            "top_p": 0.8,                    # 確保高概率輸出
            "top_k": 40,                     # 適當限制選擇範圍
            "repetition_penalty": 1.1        # 輕微懲罰重複
        }
    # 默認參數（針對一般查詢）
    else:
        return {
            "do_sample": True,
            "max_new_tokens": 400,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.1
        }


# Loop over each data entry in the test set
for index,entry in enumerate(test_data):
    entry_id = entry.get("id", "unknown_id")

    # Build the messages list from the human conversation entries
    # (Test set is expected to have only "human" messages)
    messages = []
    for conv in entry.get("conversations", []):
        if conv.get("from") == "human":
            messages.append({"role": "user", "content": conv.get("value", "")})
        else:
            messages.append({"role": "assistant", "content": conv.get("value", "")})

    # Create inputs using the chat template (required for generation)
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,  # Must add for generation
        return_tensors="pt",
    ).to("cuda")


################# TODO : Tweak Decoding Parameters here.  #####################


    # # Generate model outputs
    # outputs = model.generate(
    #     input_ids=inputs,
    #     do_sample=True,
    #     max_new_tokens=150,
    #     use_cache=True,
    #     temperature=0.7,
    #     top_p = 0.95,
    #     top_k = 50,
    # )

    # 決定最適合此查詢的生成參數
    gen_params = get_dynamic_generation_params(messages)
    
    # 使用動態參數生成回答
    outputs = model.generate(
        input_ids=inputs,
        use_cache=True,
        **gen_params
    )


################# TODO  ##########################################################

    # Decode the generated tokens
    decoded_outputs = tokenizer.batch_decode(outputs)

    # Parse each output to extract the true assistant response
    parsed_outputs = [parse_true_output(output) for output in decoded_outputs]

    # Store the result for the current entry
    inference_results[entry_id] = {
        "input": messages,
        "output": parsed_outputs
    }

    print(f"Inference completed for entry {entry_id}")


#Write the inference results to the prediction JSON file
with open(f"pred.json", "w") as outfile:
    json.dump(inference_results, outfile, indent=4)
with open(f"training_config.json", "w") as outfile:
    json.dump(training_config, outfile, indent=4)

# from google.colab import files
# files.download('./pred.json')

print("Inference completed for all entries in the test set.")

"""## Saving, loading finetuned models

### Save the model
"""

model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")

"""### Load the model"""

from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "lora_model", # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference